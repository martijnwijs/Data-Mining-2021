{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>no</th>\n",
       "      <th>target</th>\n",
       "      <th>t</th>\n",
       "      <th>activity</th>\n",
       "      <th>appCat.builtin</th>\n",
       "      <th>appCat.communication</th>\n",
       "      <th>appCat.entertainment</th>\n",
       "      <th>appCat.finance</th>\n",
       "      <th>appCat.game</th>\n",
       "      <th>...</th>\n",
       "      <th>appCat.travel</th>\n",
       "      <th>appCat.unknown</th>\n",
       "      <th>appCat.utilities</th>\n",
       "      <th>appCat.weather</th>\n",
       "      <th>call</th>\n",
       "      <th>circumplex.arousal</th>\n",
       "      <th>circumplex.valence</th>\n",
       "      <th>mood</th>\n",
       "      <th>screen</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.085503</td>\n",
       "      <td>-0.167041</td>\n",
       "      <td>2.937521</td>\n",
       "      <td>0.217447</td>\n",
       "      <td>0.675919</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>1.925529</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>1.991383</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>2.696262</td>\n",
       "      <td>1.472881</td>\n",
       "      <td>-1.779072</td>\n",
       "      <td>-1.520105</td>\n",
       "      <td>2.110863</td>\n",
       "      <td>-0.497599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199651</td>\n",
       "      <td>-0.074355</td>\n",
       "      <td>3.375349</td>\n",
       "      <td>-0.597663</td>\n",
       "      <td>0.939058</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>3.391762</td>\n",
       "      <td>2.564240</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>-0.744797</td>\n",
       "      <td>0.979184</td>\n",
       "      <td>-0.496733</td>\n",
       "      <td>-0.356840</td>\n",
       "      <td>-0.163686</td>\n",
       "      <td>0.147818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.242751</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>2.814858</td>\n",
       "      <td>0.157808</td>\n",
       "      <td>1.211872</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>0.105682</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>-0.744797</td>\n",
       "      <td>-0.172776</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>-0.589493</td>\n",
       "      <td>2.420601</td>\n",
       "      <td>-0.497599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.902421</td>\n",
       "      <td>0.035517</td>\n",
       "      <td>1.921796</td>\n",
       "      <td>-0.085389</td>\n",
       "      <td>0.914503</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151655</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>1.995236</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>-0.056586</td>\n",
       "      <td>0.485487</td>\n",
       "      <td>0.272670</td>\n",
       "      <td>0.030914</td>\n",
       "      <td>2.649757</td>\n",
       "      <td>0.147818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.199651</td>\n",
       "      <td>-0.074355</td>\n",
       "      <td>3.375349</td>\n",
       "      <td>-0.597663</td>\n",
       "      <td>0.939058</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>3.391762</td>\n",
       "      <td>2.564240</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>-0.744797</td>\n",
       "      <td>0.979184</td>\n",
       "      <td>-0.496733</td>\n",
       "      <td>-0.356840</td>\n",
       "      <td>-0.163686</td>\n",
       "      <td>0.147818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>4183</td>\n",
       "      <td>1046</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.328753</td>\n",
       "      <td>-0.065428</td>\n",
       "      <td>-0.559067</td>\n",
       "      <td>-0.041053</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>-0.327129</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>2.696262</td>\n",
       "      <td>-0.501907</td>\n",
       "      <td>-1.779072</td>\n",
       "      <td>-2.450716</td>\n",
       "      <td>2.983480</td>\n",
       "      <td>1.438652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>4184</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.035500</td>\n",
       "      <td>-0.121780</td>\n",
       "      <td>-0.111554</td>\n",
       "      <td>0.354719</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>-0.327129</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>-0.744797</td>\n",
       "      <td>-1.077887</td>\n",
       "      <td>-1.137903</td>\n",
       "      <td>-0.899697</td>\n",
       "      <td>1.171998</td>\n",
       "      <td>-0.497599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>4185</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.027631</td>\n",
       "      <td>0.850696</td>\n",
       "      <td>-0.414676</td>\n",
       "      <td>-0.484639</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>0.930922</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>-0.056586</td>\n",
       "      <td>-1.077887</td>\n",
       "      <td>-0.924180</td>\n",
       "      <td>-1.132350</td>\n",
       "      <td>0.377063</td>\n",
       "      <td>0.147818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>4186</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.328753</td>\n",
       "      <td>-0.065428</td>\n",
       "      <td>-0.559067</td>\n",
       "      <td>-0.041053</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>-0.327129</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>2.696262</td>\n",
       "      <td>-0.501907</td>\n",
       "      <td>-1.779072</td>\n",
       "      <td>-2.450716</td>\n",
       "      <td>2.983480</td>\n",
       "      <td>1.438652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>4187</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.058736</td>\n",
       "      <td>-0.245614</td>\n",
       "      <td>-0.192281</td>\n",
       "      <td>-0.546996</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.259976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.415500</td>\n",
       "      <td>-0.322651</td>\n",
       "      <td>0.401548</td>\n",
       "      <td>-0.23013</td>\n",
       "      <td>-0.400691</td>\n",
       "      <td>-0.831038</td>\n",
       "      <td>-0.753201</td>\n",
       "      <td>-1.209901</td>\n",
       "      <td>-0.354860</td>\n",
       "      <td>0.793235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    no  target  t  activity  appCat.builtin  \\\n",
       "12            12     4       6  0 -0.085503       -0.167041   \n",
       "13            13     4       6  1  0.199651       -0.074355   \n",
       "14            14     4       6  2  0.242751        0.000956   \n",
       "15            15     4       6  3  0.902421        0.035517   \n",
       "16            16     5       8  0  0.199651       -0.074355   \n",
       "...          ...   ...     ... ..       ...             ...   \n",
       "4183        4183  1046       6  3 -0.328753       -0.065428   \n",
       "4184        4184  1047       8  0 -1.035500       -0.121780   \n",
       "4185        4185  1047       8  1  1.027631        0.850696   \n",
       "4186        4186  1047       8  2 -0.328753       -0.065428   \n",
       "4187        4187  1047       8  3 -1.058736       -0.245614   \n",
       "\n",
       "      appCat.communication  appCat.entertainment  appCat.finance  appCat.game  \\\n",
       "12                2.937521              0.217447        0.675919    -0.259976   \n",
       "13                3.375349             -0.597663        0.939058    -0.259976   \n",
       "14                2.814858              0.157808        1.211872    -0.259976   \n",
       "15                1.921796             -0.085389        0.914503    -0.259976   \n",
       "16                3.375349             -0.597663        0.939058    -0.259976   \n",
       "...                    ...                   ...             ...          ...   \n",
       "4183             -0.559067             -0.041053       -0.327054    -0.259976   \n",
       "4184             -0.111554              0.354719       -0.327054    -0.259976   \n",
       "4185             -0.414676             -0.484639       -0.327054    -0.259976   \n",
       "4186             -0.559067             -0.041053       -0.327054    -0.259976   \n",
       "4187             -0.192281             -0.546996       -0.327054    -0.259976   \n",
       "\n",
       "      ...  appCat.travel  appCat.unknown  appCat.utilities  appCat.weather  \\\n",
       "12    ...       1.925529       -0.322651          1.991383        -0.23013   \n",
       "13    ...      -0.415500        3.391762          2.564240        -0.23013   \n",
       "14    ...      -0.415500       -0.322651          0.105682        -0.23013   \n",
       "15    ...      -0.151655       -0.322651          1.995236        -0.23013   \n",
       "16    ...      -0.415500        3.391762          2.564240        -0.23013   \n",
       "...   ...            ...             ...               ...             ...   \n",
       "4183  ...      -0.415500       -0.322651         -0.327129        -0.23013   \n",
       "4184  ...      -0.415500       -0.322651         -0.327129        -0.23013   \n",
       "4185  ...      -0.415500       -0.322651          0.930922        -0.23013   \n",
       "4186  ...      -0.415500       -0.322651         -0.327129        -0.23013   \n",
       "4187  ...      -0.415500       -0.322651          0.401548        -0.23013   \n",
       "\n",
       "          call  circumplex.arousal  circumplex.valence      mood    screen  \\\n",
       "12    2.696262            1.472881           -1.779072 -1.520105  2.110863   \n",
       "13   -0.744797            0.979184           -0.496733 -0.356840 -0.163686   \n",
       "14   -0.744797           -0.172776           -0.240265 -0.589493  2.420601   \n",
       "15   -0.056586            0.485487            0.272670  0.030914  2.649757   \n",
       "16   -0.744797            0.979184           -0.496733 -0.356840 -0.163686   \n",
       "...        ...                 ...                 ...       ...       ...   \n",
       "4183  2.696262           -0.501907           -1.779072 -2.450716  2.983480   \n",
       "4184 -0.744797           -1.077887           -1.137903 -0.899697  1.171998   \n",
       "4185 -0.056586           -1.077887           -0.924180 -1.132350  0.377063   \n",
       "4186  2.696262           -0.501907           -1.779072 -2.450716  2.983480   \n",
       "4187 -0.400691           -0.831038           -0.753201 -1.209901 -0.354860   \n",
       "\n",
       "           sms  \n",
       "12   -0.497599  \n",
       "13    0.147818  \n",
       "14   -0.497599  \n",
       "15    0.147818  \n",
       "16    0.147818  \n",
       "...        ...  \n",
       "4183  1.438652  \n",
       "4184 -0.497599  \n",
       "4185  0.147818  \n",
       "4186  1.438652  \n",
       "4187  0.793235  \n",
       "\n",
       "[2088 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = \"dataframe_standardized_outliers_removed_classes.csv\" \n",
    "#dataset = \"dataframe_new.csv\"  ## debug\n",
    "df = pd.read_csv(dataset) # dataframe in pandas\n",
    "\n",
    "\n",
    "df = df[df['target'] != 7] # debug check if it learns to predict not only 7\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, no, target, t, activity, appCat.builtin, appCat.communication, appCat.entertainment, appCat.finance, appCat.game, appCat.office, appCat.other, appCat.social, appCat.travel, appCat.unknown, appCat.utilities, appCat.weather, call, circumplex.arousal, circumplex.valence, mood, screen, sms]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, no, target, t, activity, appCat.builtin, appCat.communication, appCat.entertainment, appCat.finance, appCat.game, appCat.office, appCat.other, appCat.social, appCat.travel, appCat.unknown, appCat.utilities, appCat.weather, call, circumplex.arousal, circumplex.valence, mood, screen, sms]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[df['no'] == 723])\n",
    "for i in range(710, 730):  ### debug\n",
    "    df = df[df['no'] != i]\n",
    "print(df[df['no'] == 723])\n",
    "df = df.sample(frac=1) # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 4, 5, 8, 6, 8, 8, 6, 8, 6, 8, 6, 6, 9, 8, 8, 6, 6, 8, 6, 6,\n",
       "       8, 6, 6, 6, 5, 6, 5, 6, 6, 8, 6, 5, 8, 8, 6, 6, 6, 8, 8, 5, 8, 6,\n",
       "       6, 8, 8, 8, 8, 8, 8, 6, 8, 8, 6, 5, 6, 6, 6, 8, 8, 9, 5, 8, 8, 6,\n",
       "       4, 8, 8, 8, 8, 8, 8, 6, 8, 6, 8, 8, 6, 8, 8, 8, 8, 6, 8, 4, 8, 8,\n",
       "       6, 6, 6, 8, 8, 8, 6, 8, 8, 8, 6, 6, 8, 6, 8, 6, 8, 8, 6, 9, 8, 8,\n",
       "       4, 9, 6, 6, 6, 6, 8, 5, 8, 8, 8, 6, 8, 8, 4, 6, 8, 8, 6, 8, 8, 8,\n",
       "       8, 6, 6, 8, 8, 6, 6, 8, 8, 6, 8, 6, 8, 6, 8, 8, 6, 8, 8, 8, 5, 8,\n",
       "       8, 8, 6, 6, 6, 9, 6, 8, 6, 8, 8, 6, 8, 6, 6, 8, 6, 6, 6, 6, 3, 8,\n",
       "       6, 6, 6, 8, 8, 8, 6, 8, 5, 8, 6, 6, 6, 8, 8, 6, 4, 8, 8, 5, 6, 8,\n",
       "       8, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 8, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "       8, 6, 8, 6, 6, 5, 8, 6, 8, 8, 6, 6, 6, 6, 8, 6, 6, 8, 8, 6, 6, 5,\n",
       "       6, 8, 6, 6, 6, 6, 8, 6, 8, 8, 5, 6, 6, 6, 8, 8, 6, 8, 6, 6, 8, 8,\n",
       "       6, 8, 8, 8, 8, 6, 8, 6, 9, 8, 8, 8, 8, 8, 8, 6, 5, 6, 8, 6, 8, 6,\n",
       "       6, 8, 9, 8, 8, 8, 6, 8, 8, 6, 8, 6, 8, 8, 8, 5, 8, 5, 8, 6, 8, 8,\n",
       "       6, 6, 8, 6, 5, 6, 6, 6, 6, 6, 6, 8, 8, 4, 8, 8, 8, 6, 6, 8, 8, 6,\n",
       "       8, 8, 5, 4, 8, 8, 6, 6, 6, 6, 8, 8, 6, 8, 8, 8, 6, 6, 6, 6, 6, 6,\n",
       "       6, 8, 8, 4, 8, 8, 6, 6, 6, 8, 8, 8, 8, 8, 6, 8, 8, 6, 6, 8, 8, 6,\n",
       "       6, 8, 8, 6, 6, 5, 6, 8, 8, 6, 8, 6, 8, 6, 8, 3, 8, 6, 5, 8, 8, 6,\n",
       "       8, 6, 8, 6, 6, 6, 6, 6, 6, 8, 8, 6, 8, 6, 6, 6, 5, 6, 5, 6, 8, 6,\n",
       "       5, 8, 8, 9, 8, 8, 5, 8, 8, 8, 6, 8, 6, 6, 6, 8, 8, 6, 6, 8, 8, 8,\n",
       "       6, 8, 6, 6, 8, 3, 8, 6, 6, 8, 6, 8, 8, 8, 6, 6, 6, 8, 8, 8, 8, 8,\n",
       "       6, 6, 6, 6, 8, 8, 6, 6, 8, 6, 4, 8, 8, 8, 6, 8, 8, 6, 6, 6, 6, 6,\n",
       "       6, 6, 8, 8, 8, 8, 8, 6, 8, 8, 6, 8, 8, 8, 5, 8, 6, 8, 8, 8, 6, 8,\n",
       "       8, 5, 6, 6, 6], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels\n",
    "Y = df['target'].to_numpy()\n",
    "Y = Y[::4]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input \n",
    "X = df.iloc[:, 3:].to_numpy()\n",
    "X = X[:, 1:]\n",
    "split = len(X[:, 0]) / 4\n",
    "X = np.array_split(X, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "def batch_generator(X, Y, batch_size):\n",
    "    batchesx = []\n",
    "    batchesy = []\n",
    "    batchx = [] # batches\n",
    "    batchy = []\n",
    "    for i in range(len(X)):\n",
    "        batchx.append(X[i]) # add to batch\n",
    "        batchy.append(Y[i])\n",
    "        #print(batchy)\n",
    "        if i % batch_size == 0: # batch full?\n",
    "            batchesx.append([batchx])\n",
    "            batchesy.append([batchy])\n",
    "            batchx = [] # batches\n",
    "            batchy = []\n",
    "    print(batchesy)\n",
    "    return batchesx, batchesy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to torch tensor\n",
    "x_train = torch.tensor(x_train)\n",
    "#x_train = x_train[:724]#### debug\n",
    "#for i in range(710, 730):  ### debug\n",
    "    #np.delete(x_train, i)\n",
    "    #np.delete(y_train, i)\n",
    "#x_train = x_train[4:] ## debug\n",
    "y_train = torch.tensor(y_train).to(torch.float)\n",
    "#print(x_train[724])\n",
    "#print(y_train[724])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train[728])\n",
    "#print(y_train[728])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# training on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class lstm(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size, num_layers, seq_length, output_size):\n",
    "        super(). __init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.ltsm = torch.nn.LSTM(self.input_size, self.hidden_size, batch_first=True) \n",
    "        #self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.lin1 = nn.Linear(self.hidden_size, self.output_size) # 1 for regression, 10 for classification\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.ltsm(x)\n",
    "        #x  = self.dropout(x)\n",
    "        x = F.relu(x) # is this ok?\n",
    "        x = self.lin1(x)  \n",
    "        return x, (hn, cn)\n",
    "net = lstm(input_size=19, hidden_size=60, num_layers=1, seq_length=4, output_size=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.0001\n",
    "criterion =  nn.MSELoss()    # MSE for regression\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(0, 10, (10,))\n",
    "one_hot = torch.nn.functional.one_hot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 40.626\n",
      "[1,   200] loss: 36.968\n",
      "[1,   300] loss: 30.570\n",
      "[1,   400] loss: 21.936\n",
      "[2,   100] loss: 15.647\n",
      "[2,   200] loss: 10.699\n",
      "[2,   300] loss: 6.898\n",
      "[2,   400] loss: 4.518\n",
      "[3,   100] loss: 3.537\n",
      "[3,   200] loss: 2.939\n",
      "[3,   300] loss: 2.027\n",
      "[3,   400] loss: 1.809\n",
      "[4,   100] loss: 1.709\n",
      "[4,   200] loss: 1.749\n",
      "[4,   300] loss: 1.288\n",
      "[4,   400] loss: 1.508\n",
      "[5,   100] loss: 1.466\n",
      "[5,   200] loss: 1.576\n",
      "[5,   300] loss: 1.182\n",
      "[5,   400] loss: 1.493\n",
      "[6,   100] loss: 1.428\n",
      "[6,   200] loss: 1.534\n",
      "[6,   300] loss: 1.153\n",
      "[6,   400] loss: 1.488\n",
      "[7,   100] loss: 1.408\n",
      "[7,   200] loss: 1.503\n",
      "[7,   300] loss: 1.128\n",
      "[7,   400] loss: 1.478\n",
      "[8,   100] loss: 1.388\n",
      "[8,   200] loss: 1.465\n",
      "[8,   300] loss: 1.101\n",
      "[8,   400] loss: 1.468\n",
      "[9,   100] loss: 1.366\n",
      "[9,   200] loss: 1.419\n",
      "[9,   300] loss: 1.071\n",
      "[9,   400] loss: 1.462\n",
      "[10,   100] loss: 1.344\n",
      "[10,   200] loss: 1.368\n",
      "[10,   300] loss: 1.042\n",
      "[10,   400] loss: 1.462\n",
      "[11,   100] loss: 1.321\n",
      "[11,   200] loss: 1.318\n",
      "[11,   300] loss: 1.017\n",
      "[11,   400] loss: 1.460\n",
      "[12,   100] loss: 1.295\n",
      "[12,   200] loss: 1.273\n",
      "[12,   300] loss: 0.993\n",
      "[12,   400] loss: 1.451\n",
      "[13,   100] loss: 1.266\n",
      "[13,   200] loss: 1.232\n",
      "[13,   300] loss: 0.968\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b4c33f265add>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m#print(i, loss.item())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# running loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'betas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "n = 10 # for one hot\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i in range(len(x_train)):\n",
    "        \n",
    "        # get the inputs\n",
    "        #print(torch.unsqueeze(x_train[i], 0))\n",
    "        inputs, labels =torch.unsqueeze(x_train[i], 0).to(device).float(), torch.unsqueeze(y_train[i], 0).to(device)\n",
    "        #labels_onehot = torch.nn.functional.one_hot(labels[0].to(torch.int64), n)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, (hn, cn) = net(inputs)\n",
    "        #print(outputs)\n",
    "        #output = torch.mean(outputs, dim=1) # takes the average over the outputs \n",
    "        output = outputs[0][3] # takes the last, (does that makes more sense)\n",
    "        \n",
    "        #print(output)\n",
    "        #print(\"2222\", output2)\n",
    "        #print(labels[0])\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        #print(i, loss.item())\n",
    "        # running loss\n",
    "        running_loss += loss.item()\n",
    "        #print(i, loss.item())\n",
    "        # statistics tensorboard\n",
    "        if i % 100 == 99:    # every 30 mini-batches\n",
    "\n",
    "            # print\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            \n",
    "            '''\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 30,\n",
    "                            epoch * len(x_train) + i)\n",
    "            '''\n",
    "            running_loss = 0.0\n",
    "'''\n",
    "    # run on validation set\n",
    "    net.eval()\n",
    "    for j, data in enumerate(x_val, 0):\n",
    "        inputs, labels = x_val[j].to(device), y_val[j].to(device)\n",
    "        \n",
    "        # calculate outputs and loss\n",
    "        outputs, (hn, cn) = net(inputs)\n",
    "        output = torch.mean(outputs, dim=1)  \n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # add to tensorboard\n",
    "    writer.add_scalar('validation_loss',\n",
    "                            running_loss / j,\n",
    "                            epoch)\n",
    "    print(\"validation loss:\", running_loss / j, j)\n",
    "'''\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(5.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(6.)\n",
      "labels tensor(5.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(5.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(9.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(4.)\n",
      "labels tensor(5.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(6.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(6.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(3.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(6.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(6.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(4.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(6.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(6.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(5.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(8.)\n",
      "output tensor(7.)\n",
      "labels tensor(5.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "output tensor(7.)\n",
      "labels tensor(6.)\n",
      "Accuracy on the test set: 4 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for i in range(len(x_test)):\n",
    "        inputs, labels =torch.unsqueeze(x_test[i], 0).to(device).float(), torch.unsqueeze(y_test[i], 0).to(device)\n",
    "        outputs, (hn, cn) = net(inputs)\n",
    "        #output = torch.mean(outputs, dim=1)\n",
    "        output = outputs[0][3]\n",
    "        #print(inputs)\n",
    "        outputr = torch.round(output[0])\n",
    "        print('output', outputr)\n",
    "        #_, predicted = torch.max(output.data, 1)\n",
    "        #print(predicted)\n",
    "        print(\"labels\", labels[0])\n",
    "        if labels[0] == outputr:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "final = 100 * correct / total\n",
    "print('Accuracy on the test set: %d %%' % (final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
