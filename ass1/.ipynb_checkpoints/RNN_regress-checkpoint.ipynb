{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>no</th>\n",
       "      <th>target</th>\n",
       "      <th>t</th>\n",
       "      <th>activity</th>\n",
       "      <th>appCat.builtin</th>\n",
       "      <th>appCat.communication</th>\n",
       "      <th>appCat.entertainment</th>\n",
       "      <th>appCat.finance</th>\n",
       "      <th>appCat.game</th>\n",
       "      <th>...</th>\n",
       "      <th>appCat.travel</th>\n",
       "      <th>appCat.unknown</th>\n",
       "      <th>appCat.utilities</th>\n",
       "      <th>appCat.weather</th>\n",
       "      <th>call</th>\n",
       "      <th>circumplex.arousal</th>\n",
       "      <th>circumplex.valence</th>\n",
       "      <th>mood</th>\n",
       "      <th>screen</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.164281</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.447451</td>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.025816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.415626</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.204670</td>\n",
       "      <td>0.674451</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.032589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083367</td>\n",
       "      <td>0.098729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.593175</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.210775</td>\n",
       "      <td>0.675567</td>\n",
       "      <td>0.433846</td>\n",
       "      <td>0.025335</td>\n",
       "      <td>0.039612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.453645</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304211</td>\n",
       "      <td>0.676080</td>\n",
       "      <td>0.334794</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.031957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.481773</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.204670</td>\n",
       "      <td>0.674451</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.032589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083367</td>\n",
       "      <td>0.098729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.593175</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>4183</td>\n",
       "      <td>1046</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.129827</td>\n",
       "      <td>0.674583</td>\n",
       "      <td>0.059634</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.522736</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>4184</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029723</td>\n",
       "      <td>0.673748</td>\n",
       "      <td>0.109269</td>\n",
       "      <td>0.031444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.300383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>4185</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321946</td>\n",
       "      <td>0.688163</td>\n",
       "      <td>0.075649</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.202808</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>4186</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.129827</td>\n",
       "      <td>0.674583</td>\n",
       "      <td>0.059634</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.522736</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>4187</td>\n",
       "      <td>1047</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>0.671912</td>\n",
       "      <td>0.100315</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.112967</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    no  target  t  activity  appCat.builtin  \\\n",
       "12            12     4       6  0  0.164281        0.673077   \n",
       "13            13     4       6  1  0.204670        0.674451   \n",
       "14            14     4       6  2  0.210775        0.675567   \n",
       "15            15     4       6  3  0.304211        0.676080   \n",
       "16            16     5       8  0  0.204670        0.674451   \n",
       "...          ...   ...     ... ..       ...             ...   \n",
       "4183        4183  1046       6  3  0.129827        0.674583   \n",
       "4184        4184  1047       8  0  0.029723        0.673748   \n",
       "4185        4185  1047       8  1  0.321946        0.688163   \n",
       "4186        4186  1047       8  2  0.129827        0.674583   \n",
       "4187        4187  1047       8  3  0.026432        0.671912   \n",
       "\n",
       "      appCat.communication  appCat.entertainment  appCat.finance  appCat.game  \\\n",
       "12                0.447451              0.027185        0.025816          0.0   \n",
       "13                0.496012              0.001898        0.032589          0.0   \n",
       "14                0.433846              0.025335        0.039612          0.0   \n",
       "15                0.334794              0.017790        0.031957          0.0   \n",
       "16                0.496012              0.001898        0.032589          0.0   \n",
       "...                    ...                   ...             ...          ...   \n",
       "4183              0.059634              0.019166        0.000000          0.0   \n",
       "4184              0.109269              0.031444        0.000000          0.0   \n",
       "4185              0.075649              0.005404        0.000000          0.0   \n",
       "4186              0.059634              0.019166        0.000000          0.0   \n",
       "4187              0.100315              0.003470        0.000000          0.0   \n",
       "\n",
       "      ...  appCat.travel  appCat.unknown  appCat.utilities  appCat.weather  \\\n",
       "12    ...       0.051229        0.000000          0.079168             0.0   \n",
       "13    ...       0.000000        0.083367          0.098729             0.0   \n",
       "14    ...       0.000000        0.000000          0.014779             0.0   \n",
       "15    ...       0.005774        0.000000          0.079300             0.0   \n",
       "16    ...       0.000000        0.083367          0.098729             0.0   \n",
       "...   ...            ...             ...               ...             ...   \n",
       "4183  ...       0.000000        0.000000          0.000000             0.0   \n",
       "4184  ...       0.000000        0.000000          0.000000             0.0   \n",
       "4185  ...       0.000000        0.000000          0.042958             0.0   \n",
       "4186  ...       0.000000        0.000000          0.000000             0.0   \n",
       "4187  ...       0.000000        0.000000          0.024881             0.0   \n",
       "\n",
       "          call  circumplex.arousal  circumplex.valence      mood    screen  \\\n",
       "12    0.322581              0.7000            0.384615  0.473684  0.415626   \n",
       "13    0.000000              0.6250            0.538462  0.592105  0.593175   \n",
       "14    0.000000              0.4500            0.569231  0.568421  0.453645   \n",
       "15    0.064516              0.5500            0.630769  0.631579  0.481773   \n",
       "16    0.000000              0.6250            0.538462  0.592105  0.593175   \n",
       "...        ...                 ...                 ...       ...       ...   \n",
       "4183  0.322581              0.4000            0.384615  0.378947  0.522736   \n",
       "4184  0.000000              0.3125            0.461538  0.536842  0.300383   \n",
       "4185  0.064516              0.3125            0.487179  0.513158  0.202808   \n",
       "4186  0.322581              0.4000            0.384615  0.378947  0.522736   \n",
       "4187  0.032258              0.3500            0.507692  0.505263  0.112967   \n",
       "\n",
       "           sms  \n",
       "12    0.000000  \n",
       "13    0.066667  \n",
       "14    0.000000  \n",
       "15    0.066667  \n",
       "16    0.066667  \n",
       "...        ...  \n",
       "4183  0.200000  \n",
       "4184  0.000000  \n",
       "4185  0.066667  \n",
       "4186  0.200000  \n",
       "4187  0.133333  \n",
       "\n",
       "[2088 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = \"dataframe_normalized_outliers_removed.csv\" \n",
    "#dataset = \"dataframe_new.csv\"  ## debug\n",
    "df = pd.read_csv(dataset) # dataframe in pandas\n",
    "\n",
    "\n",
    "df = df[df['target'] != 7] # debug check if it learns to predict not only 7\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, no, target, t, activity, appCat.builtin, appCat.communication, appCat.entertainment, appCat.finance, appCat.game, appCat.office, appCat.other, appCat.social, appCat.travel, appCat.unknown, appCat.utilities, appCat.weather, call, circumplex.arousal, circumplex.valence, mood, screen, sms]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, no, target, t, activity, appCat.builtin, appCat.communication, appCat.entertainment, appCat.finance, appCat.game, appCat.office, appCat.other, appCat.social, appCat.travel, appCat.unknown, appCat.utilities, appCat.weather, call, circumplex.arousal, circumplex.valence, mood, screen, sms]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[df['no'] == 723])\n",
    "for i in range(710, 730):  ### debug\n",
    "    df = df[df['no'] != i]\n",
    "print(df[df['no'] == 723])\n",
    "df = df.sample(frac=1) # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, 6, 8, 8, 6, 6, 8, 8, 8, 8, 6, 6, 8, 8, 5, 6, 6, 6, 6, 8,\n",
       "       6, 6, 8, 6, 8, 6, 8, 6, 6, 8, 8, 8, 6, 8, 8, 6, 6, 8, 8, 6, 8, 6,\n",
       "       8, 6, 8, 4, 6, 6, 6, 6, 6, 8, 8, 8, 6, 6, 8, 6, 8, 8, 8, 8, 6, 8,\n",
       "       8, 6, 6, 6, 6, 6, 6, 8, 8, 6, 8, 8, 8, 6, 5, 6, 6, 8, 8, 6, 8, 8,\n",
       "       6, 6, 8, 6, 8, 8, 8, 8, 6, 5, 6, 6, 8, 6, 8, 6, 8, 6, 6, 8, 8, 8,\n",
       "       8, 5, 8, 5, 4, 8, 6, 8, 8, 6, 8, 5, 8, 6, 8, 6, 8, 8, 8, 6, 8, 8,\n",
       "       8, 6, 8, 8, 6, 6, 6, 6, 8, 6, 6, 8, 8, 8, 6, 8, 8, 8, 8, 8, 8, 6,\n",
       "       8, 6, 8, 6, 8, 8, 8, 5, 6, 8, 6, 8, 6, 8, 6, 4, 6, 5, 8, 9, 6, 8,\n",
       "       6, 8, 8, 8, 8, 8, 8, 6, 6, 6, 8, 8, 8, 6, 8, 6, 6, 5, 8, 6, 8, 8,\n",
       "       6, 6, 9, 6, 5, 8, 8, 6, 8, 6, 6, 8, 6, 4, 8, 6, 9, 8, 8, 6, 6, 8,\n",
       "       8, 6, 8, 8, 6, 6, 6, 6, 5, 8, 6, 6, 6, 8, 9, 8, 6, 8, 6, 6, 8, 8,\n",
       "       6, 6, 6, 8, 6, 8, 6, 8, 6, 8, 8, 8, 9, 5, 6, 8, 8, 8, 8, 6, 6, 8,\n",
       "       6, 6, 6, 6, 8, 8, 8, 8, 6, 8, 6, 8, 6, 8, 8, 6, 8, 8, 8, 8, 8, 8,\n",
       "       6, 8, 8, 6, 8, 8, 6, 8, 6, 8, 6, 8, 6, 6, 8, 4, 8, 8, 8, 6, 6, 6,\n",
       "       6, 4, 6, 6, 6, 6, 8, 6, 6, 8, 8, 6, 5, 8, 8, 8, 6, 5, 8, 8, 8, 6,\n",
       "       6, 6, 6, 6, 8, 8, 6, 6, 6, 6, 8, 8, 6, 8, 8, 6, 8, 6, 5, 5, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 8, 4, 8, 6, 6, 8, 4, 6, 6, 6, 8, 8, 8, 8, 6,\n",
       "       8, 6, 8, 5, 8, 6, 8, 8, 8, 4, 8, 6, 8, 6, 8, 8, 6, 6, 8, 6, 8, 6,\n",
       "       6, 6, 8, 8, 5, 8, 8, 8, 8, 8, 8, 3, 8, 6, 6, 6, 8, 6, 6, 8, 8, 6,\n",
       "       5, 6, 6, 8, 6, 5, 6, 6, 8, 8, 8, 5, 8, 8, 6, 8, 8, 8, 8, 8, 8, 6,\n",
       "       8, 8, 6, 8, 6, 8, 6, 6, 6, 8, 6, 8, 6, 8, 5, 8, 8, 6, 8, 6, 8, 6,\n",
       "       6, 6, 6, 6, 8, 8, 4, 8, 8, 8, 6, 6, 8, 6, 8, 8, 6, 8, 6, 8, 8, 8,\n",
       "       6, 8, 8, 8, 8, 6, 6, 6, 6, 6, 8, 8, 8, 8, 6, 8, 6, 8, 3, 8, 8, 8,\n",
       "       8, 6, 8, 8, 6], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels\n",
    "Y = df['target'].to_numpy()\n",
    "Y = Y[::4]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input \n",
    "X = df.iloc[:, 3:].to_numpy()\n",
    "X = X[:, 1:]\n",
    "split = len(X[:, 0]) / 4\n",
    "X = np.array_split(X, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "def batch_generator(X, Y, batch_size):\n",
    "    batchesx = []\n",
    "    batchesy = []\n",
    "    batchx = [] # batches\n",
    "    batchy = []\n",
    "    for i in range(len(X)):\n",
    "        batchx.append(X[i]) # add to batch\n",
    "        batchy.append(Y[i])\n",
    "        #print(batchy)\n",
    "        if i % batch_size == 0: # batch full?\n",
    "            batchesx.append([batchx])\n",
    "            batchesy.append([batchy])\n",
    "            batchx = [] # batches\n",
    "            batchy = []\n",
    "    print(batchesy)\n",
    "    return batchesx, batchesy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to torch tensor\n",
    "x_train = torch.tensor(x_train)\n",
    "#x_train = x_train[:724]#### debug\n",
    "#for i in range(710, 730):  ### debug\n",
    "    #np.delete(x_train, i)\n",
    "    #np.delete(y_train, i)\n",
    "#x_train = x_train[4:] ## debug\n",
    "y_train = torch.tensor(y_train).to(torch.float)\n",
    "#print(x_train[724])\n",
    "#print(y_train[724])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train[728])\n",
    "#print(y_train[728])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# training on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class lstm(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size, num_layers, seq_length, output_size):\n",
    "        super(). __init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.ltsm = torch.nn.LSTM(self.input_size, self.hidden_size, batch_first=True) \n",
    "        #self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.lin1 = nn.Linear(self.hidden_size, self.output_size) # 1 for regression, 10 for classification\n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.ltsm(x)\n",
    "        #x  = self.dropout(x)\n",
    "        x = F.relu(x) # is this ok?\n",
    "        x = self.lin1(x)  \n",
    "        return x, (hn, cn)\n",
    "net = lstm(input_size=19, hidden_size=60, num_layers=1, seq_length=4, output_size=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.00001\n",
    "criterion =  nn.MSELoss()    # MSE for regression\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(0, 10, (10,))\n",
    "one_hot = torch.nn.functional.one_hot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 48.252\n",
      "[1,   200] loss: 49.737\n",
      "[1,   300] loss: 49.823\n",
      "[1,   400] loss: 44.707\n",
      "[2,   100] loss: 47.778\n",
      "[2,   200] loss: 49.229\n",
      "[2,   300] loss: 49.276\n",
      "[2,   400] loss: 44.148\n",
      "[3,   100] loss: 47.145\n",
      "[3,   200] loss: 48.524\n",
      "[3,   300] loss: 48.498\n",
      "[3,   400] loss: 43.337\n",
      "[4,   100] loss: 46.210\n",
      "[4,   200] loss: 47.467\n",
      "[4,   300] loss: 47.311\n",
      "[4,   400] loss: 42.080\n",
      "[5,   100] loss: 44.728\n",
      "[5,   200] loss: 45.769\n",
      "[5,   300] loss: 45.390\n",
      "[5,   400] loss: 40.036\n",
      "[6,   100] loss: 42.325\n",
      "[6,   200] loss: 43.046\n",
      "[6,   300] loss: 42.360\n",
      "[6,   400] loss: 36.901\n",
      "[7,   100] loss: 38.776\n",
      "[7,   200] loss: 39.215\n",
      "[7,   300] loss: 38.330\n",
      "[7,   400] loss: 32.987\n",
      "[8,   100] loss: 34.637\n",
      "[8,   200] loss: 35.024\n",
      "[8,   300] loss: 34.188\n",
      "[8,   400] loss: 29.206\n",
      "[9,   100] loss: 30.838\n",
      "[9,   200] loss: 31.330\n",
      "[9,   300] loss: 30.661\n",
      "[9,   400] loss: 26.091\n",
      "[10,   100] loss: 27.762\n",
      "[10,   200] loss: 28.370\n",
      "[10,   300] loss: 27.859\n",
      "[10,   400] loss: 23.642\n",
      "[11,   100] loss: 25.336\n",
      "[11,   200] loss: 26.029\n",
      "[11,   300] loss: 25.635\n",
      "[11,   400] loss: 21.700\n",
      "[12,   100] loss: 23.394\n",
      "[12,   200] loss: 24.141\n",
      "[12,   300] loss: 23.829\n",
      "[12,   400] loss: 20.118\n",
      "[13,   100] loss: 21.795\n",
      "[13,   200] loss: 22.573\n",
      "[13,   300] loss: 22.319\n",
      "[13,   400] loss: 18.791\n",
      "[14,   100] loss: 20.439\n",
      "[14,   200] loss: 21.233\n",
      "[14,   300] loss: 21.018\n",
      "[14,   400] loss: 17.644\n",
      "[15,   100] loss: 19.257\n",
      "[15,   200] loss: 20.057\n",
      "[15,   300] loss: 19.871\n",
      "[15,   400] loss: 16.629\n",
      "[16,   100] loss: 18.204\n",
      "[16,   200] loss: 19.003\n",
      "[16,   300] loss: 18.837\n",
      "[16,   400] loss: 15.714\n",
      "[17,   100] loss: 17.246\n",
      "[17,   200] loss: 18.041\n",
      "[17,   300] loss: 17.890\n",
      "[17,   400] loss: 14.874\n",
      "[18,   100] loss: 16.365\n",
      "[18,   200] loss: 17.152\n",
      "[18,   300] loss: 17.013\n",
      "[18,   400] loss: 14.096\n",
      "[19,   100] loss: 15.543\n",
      "[19,   200] loss: 16.321\n",
      "[19,   300] loss: 16.191\n",
      "[19,   400] loss: 13.368\n",
      "[20,   100] loss: 14.771\n",
      "[20,   200] loss: 15.539\n",
      "[20,   300] loss: 15.415\n",
      "[20,   400] loss: 12.681\n",
      "[21,   100] loss: 14.042\n",
      "[21,   200] loss: 14.798\n",
      "[21,   300] loss: 14.680\n",
      "[21,   400] loss: 12.031\n",
      "[22,   100] loss: 13.349\n",
      "[22,   200] loss: 14.093\n",
      "[22,   300] loss: 13.979\n",
      "[22,   400] loss: 11.413\n",
      "[23,   100] loss: 12.688\n",
      "[23,   200] loss: 13.420\n",
      "[23,   300] loss: 13.310\n",
      "[23,   400] loss: 10.824\n",
      "[24,   100] loss: 12.056\n",
      "[24,   200] loss: 12.776\n",
      "[24,   300] loss: 12.669\n",
      "[24,   400] loss: 10.261\n",
      "[25,   100] loss: 11.451\n",
      "[25,   200] loss: 12.158\n",
      "[25,   300] loss: 12.054\n",
      "[25,   400] loss: 9.723\n",
      "[26,   100] loss: 10.871\n",
      "[26,   200] loss: 11.565\n",
      "[26,   300] loss: 11.463\n",
      "[26,   400] loss: 9.208\n",
      "[27,   100] loss: 10.314\n",
      "[27,   200] loss: 10.996\n",
      "[27,   300] loss: 10.896\n",
      "[27,   400] loss: 8.716\n",
      "[28,   100] loss: 9.780\n",
      "[28,   200] loss: 10.449\n",
      "[28,   300] loss: 10.351\n",
      "[28,   400] loss: 8.244\n",
      "[29,   100] loss: 9.268\n",
      "[29,   200] loss: 9.924\n",
      "[29,   300] loss: 9.827\n",
      "[29,   400] loss: 7.793\n",
      "[30,   100] loss: 8.775\n",
      "[30,   200] loss: 9.419\n",
      "[30,   300] loss: 9.324\n",
      "[30,   400] loss: 7.361\n",
      "[31,   100] loss: 8.303\n",
      "[31,   200] loss: 8.934\n",
      "[31,   300] loss: 8.840\n",
      "[31,   400] loss: 6.948\n",
      "[32,   100] loss: 7.850\n",
      "[32,   200] loss: 8.469\n",
      "[32,   300] loss: 8.376\n",
      "[32,   400] loss: 6.554\n",
      "[33,   100] loss: 7.417\n",
      "[33,   200] loss: 8.022\n",
      "[33,   300] loss: 7.931\n",
      "[33,   400] loss: 6.178\n",
      "[34,   100] loss: 7.001\n",
      "[34,   200] loss: 7.595\n",
      "[34,   300] loss: 7.505\n",
      "[34,   400] loss: 5.820\n",
      "[35,   100] loss: 6.604\n",
      "[35,   200] loss: 7.185\n",
      "[35,   300] loss: 7.097\n",
      "[35,   400] loss: 5.480\n",
      "[36,   100] loss: 6.225\n",
      "[36,   200] loss: 6.794\n",
      "[36,   300] loss: 6.706\n",
      "[36,   400] loss: 5.156\n",
      "[37,   100] loss: 5.863\n",
      "[37,   200] loss: 6.420\n",
      "[37,   300] loss: 6.333\n",
      "[37,   400] loss: 4.850\n",
      "[38,   100] loss: 5.519\n",
      "[38,   200] loss: 6.063\n",
      "[38,   300] loss: 5.978\n",
      "[38,   400] loss: 4.560\n",
      "[39,   100] loss: 5.191\n",
      "[39,   200] loss: 5.724\n",
      "[39,   300] loss: 5.639\n",
      "[39,   400] loss: 4.286\n",
      "[40,   100] loss: 4.880\n",
      "[40,   200] loss: 5.401\n",
      "[40,   300] loss: 5.317\n",
      "[40,   400] loss: 4.027\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "n = 10 # for one hot\n",
    "\n",
    "for epoch in range(40):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i in range(len(x_train)):\n",
    "        \n",
    "        # get the inputs\n",
    "        #print(torch.unsqueeze(x_train[i], 0))\n",
    "        inputs, labels =torch.unsqueeze(x_train[i], 0).to(device).float(), torch.unsqueeze(y_train[i], 0).to(device)\n",
    "        labels_onehot = torch.nn.functional.one_hot(labels[0].to(torch.int64), n)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, (hn, cn) = net(inputs)\n",
    "        #print(outputs)\n",
    "        #output = torch.mean(outputs, dim=1) # takes the average over the outputs \n",
    "        output = outputs[0][3] # takes the last, (does that makes more sense)\n",
    "        \n",
    "        #print(output)\n",
    "        #print(\"2222\", output2)\n",
    "        #print(labels[0])\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        #print(i, loss.item())\n",
    "        # running loss\n",
    "        running_loss += loss.item()\n",
    "        #print(i, loss.item())\n",
    "        # statistics tensorboard\n",
    "        if i % 100 == 99:    # every 30 mini-batches\n",
    "\n",
    "            # print\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            \n",
    "            '''\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 30,\n",
    "                            epoch * len(x_train) + i)\n",
    "            '''\n",
    "            running_loss = 0.0\n",
    "'''\n",
    "    # run on validation set\n",
    "    net.eval()\n",
    "    for j, data in enumerate(x_val, 0):\n",
    "        inputs, labels = x_val[j].to(device), y_val[j].to(device)\n",
    "        \n",
    "        # calculate outputs and loss\n",
    "        outputs, (hn, cn) = net(inputs)\n",
    "        output = torch.mean(outputs, dim=1)  \n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # add to tensorboard\n",
    "    writer.add_scalar('validation_loss',\n",
    "                            running_loss / j,\n",
    "                            epoch)\n",
    "    print(\"validation loss:\", running_loss / j, j)\n",
    "'''\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0701])\n",
      "tensor([8.])\n",
      "tensor([5.0678])\n",
      "tensor([6.])\n",
      "tensor([5.0705])\n",
      "tensor([6.])\n",
      "tensor([5.0658])\n",
      "tensor([6.])\n",
      "tensor([5.0723])\n",
      "tensor([8.])\n",
      "tensor([5.0703])\n",
      "tensor([6.])\n",
      "tensor([5.0692])\n",
      "tensor([6.])\n",
      "tensor([5.0746])\n",
      "tensor([8.])\n",
      "tensor([5.0746])\n",
      "tensor([8.])\n",
      "tensor([5.0720])\n",
      "tensor([6.])\n",
      "tensor([5.0690])\n",
      "tensor([5.])\n",
      "tensor([5.0711])\n",
      "tensor([6.])\n",
      "tensor([5.0675])\n",
      "tensor([6.])\n",
      "tensor([5.0717])\n",
      "tensor([8.])\n",
      "tensor([5.0674])\n",
      "tensor([6.])\n",
      "tensor([5.0697])\n",
      "tensor([5.])\n",
      "tensor([5.0710])\n",
      "tensor([6.])\n",
      "tensor([5.0744])\n",
      "tensor([6.])\n",
      "tensor([5.0740])\n",
      "tensor([8.])\n",
      "tensor([5.0726])\n",
      "tensor([8.])\n",
      "tensor([5.0713])\n",
      "tensor([8.])\n",
      "tensor([5.0686])\n",
      "tensor([5.])\n",
      "tensor([5.0715])\n",
      "tensor([8.])\n",
      "tensor([5.0732])\n",
      "tensor([8.])\n",
      "tensor([5.0697])\n",
      "tensor([6.])\n",
      "tensor([5.0735])\n",
      "tensor([8.])\n",
      "tensor([5.0728])\n",
      "tensor([8.])\n",
      "tensor([5.0687])\n",
      "tensor([8.])\n",
      "tensor([5.0701])\n",
      "tensor([8.])\n",
      "tensor([5.0726])\n",
      "tensor([8.])\n",
      "tensor([5.0702])\n",
      "tensor([8.])\n",
      "tensor([5.0706])\n",
      "tensor([6.])\n",
      "tensor([5.0745])\n",
      "tensor([8.])\n",
      "tensor([5.0688])\n",
      "tensor([8.])\n",
      "tensor([5.0690])\n",
      "tensor([6.])\n",
      "tensor([5.0749])\n",
      "tensor([8.])\n",
      "tensor([5.0714])\n",
      "tensor([6.])\n",
      "tensor([5.0696])\n",
      "tensor([8.])\n",
      "tensor([5.0699])\n",
      "tensor([6.])\n",
      "tensor([5.0727])\n",
      "tensor([6.])\n",
      "tensor([5.0678])\n",
      "tensor([6.])\n",
      "tensor([5.0718])\n",
      "tensor([8.])\n",
      "tensor([5.0727])\n",
      "tensor([6.])\n",
      "tensor([5.0667])\n",
      "tensor([8.])\n",
      "tensor([5.0655])\n",
      "tensor([6.])\n",
      "tensor([5.0744])\n",
      "tensor([8.])\n",
      "tensor([5.0704])\n",
      "tensor([5.])\n",
      "tensor([5.0713])\n",
      "tensor([8.])\n",
      "tensor([5.0720])\n",
      "tensor([8.])\n",
      "tensor([5.0717])\n",
      "tensor([6.])\n",
      "tensor([5.0710])\n",
      "tensor([8.])\n",
      "tensor([5.0710])\n",
      "tensor([6.])\n",
      "tensor([5.0722])\n",
      "tensor([8.])\n",
      "tensor([5.0670])\n",
      "tensor([6.])\n",
      "tensor([5.0734])\n",
      "tensor([6.])\n",
      "tensor([5.0694])\n",
      "tensor([6.])\n",
      "tensor([5.0728])\n",
      "tensor([6.])\n",
      "tensor([5.0741])\n",
      "tensor([6.])\n",
      "tensor([5.0706])\n",
      "tensor([8.])\n",
      "tensor([5.0737])\n",
      "tensor([8.])\n",
      "tensor([5.0724])\n",
      "tensor([4.])\n",
      "tensor([5.0724])\n",
      "tensor([8.])\n",
      "tensor([5.0735])\n",
      "tensor([8.])\n",
      "tensor([5.0719])\n",
      "tensor([8.])\n",
      "tensor([5.0736])\n",
      "tensor([6.])\n",
      "tensor([5.0685])\n",
      "tensor([6.])\n",
      "tensor([5.0701])\n",
      "tensor([8.])\n",
      "tensor([5.0740])\n",
      "tensor([6.])\n",
      "tensor([5.0697])\n",
      "tensor([8.])\n",
      "tensor([5.0717])\n",
      "tensor([8.])\n",
      "tensor([5.0647])\n",
      "tensor([6.])\n",
      "tensor([5.0705])\n",
      "tensor([8.])\n",
      "tensor([5.0727])\n",
      "tensor([6.])\n",
      "tensor([5.0714])\n",
      "tensor([8.])\n",
      "tensor([5.0655])\n",
      "tensor([8.])\n",
      "tensor([5.0717])\n",
      "tensor([8.])\n",
      "tensor([5.0704])\n",
      "tensor([6.])\n",
      "tensor([5.0712])\n",
      "tensor([8.])\n",
      "tensor([5.0729])\n",
      "tensor([8.])\n",
      "tensor([5.0726])\n",
      "tensor([8.])\n",
      "tensor([5.0711])\n",
      "tensor([8.])\n",
      "tensor([5.0693])\n",
      "tensor([6.])\n",
      "tensor([5.0681])\n",
      "tensor([6.])\n",
      "tensor([5.0740])\n",
      "tensor([6.])\n",
      "tensor([5.0691])\n",
      "tensor([6.])\n",
      "tensor([5.0705])\n",
      "tensor([6.])\n",
      "tensor([5.0696])\n",
      "tensor([8.])\n",
      "tensor([5.0752])\n",
      "tensor([8.])\n",
      "tensor([5.0738])\n",
      "tensor([8.])\n",
      "tensor([5.0689])\n",
      "tensor([8.])\n",
      "tensor([5.0721])\n",
      "tensor([6.])\n",
      "tensor([5.0728])\n",
      "tensor([8.])\n",
      "tensor([5.0720])\n",
      "tensor([6.])\n",
      "tensor([5.0738])\n",
      "tensor([8.])\n",
      "tensor([5.0693])\n",
      "tensor([3.])\n",
      "tensor([5.0715])\n",
      "tensor([8.])\n",
      "tensor([5.0701])\n",
      "tensor([8.])\n",
      "tensor([5.0690])\n",
      "tensor([8.])\n",
      "tensor([5.0702])\n",
      "tensor([8.])\n",
      "tensor([5.0716])\n",
      "tensor([6.])\n",
      "tensor([5.0720])\n",
      "tensor([8.])\n",
      "tensor([5.0739])\n",
      "tensor([8.])\n",
      "tensor([5.0668])\n",
      "tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for i in range(len(x_test)):\n",
    "        inputs, labels =torch.unsqueeze(x_test[i], 0).to(device).float(), torch.unsqueeze(y_test[i], 0).to(device)\n",
    "        outputs, (hn, cn) = net(inputs)\n",
    "        #output = torch.mean(outputs, dim=1)\n",
    "        output = outputs[0][3]\n",
    "        #print(inputs)\n",
    "        print(output)\n",
    "        #_, predicted = torch.max(output.data, 1)\n",
    "        #print(predicted)\n",
    "        print(labels)\n",
    "        #total += labels.size(0)\n",
    "        #correct += (predicted == labels).sum().item()\n",
    "#final = 100 * correct / total\n",
    "#print('Accuracy on the test set: %d %%' % (final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
